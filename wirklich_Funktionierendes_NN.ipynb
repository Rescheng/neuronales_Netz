{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4a3622a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anfan1g\n",
      "reset_all\n",
      "0\n",
      "[293.86728931   1.           1.           0.           0.\n",
      "   1.           0.           1.           1.           0.\n",
      "   0.         100.          10.           0.           0.        ]\n",
      "[293.86728931   1.           1.           0.           0.\n",
      "   1.           0.           1.           1.           0.\n",
      "   0.         100.          10.           0.           0.        ]\n",
      "Debug store_transition:  [293.86728931   1.           1.           0.           0.\n",
      "   1.           0.           1.           1.           0.\n",
      "   0.         100.          10.           0.           0.        ] \n",
      " self.state_memory[index]:  [0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DanielDHerzog\\AppData\\Local\\Temp\\ipykernel_14936\\1594189325.py:58: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (15,) into shape (8,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14936\\1594189325.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m         agent.store_transition(observation, action, \\\n\u001b[0m\u001b[0;32m    434\u001b[0m                                score, observation_, done)        \n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14936\\1594189325.py\u001b[0m in \u001b[0;36mstore_transition\u001b[1;34m(self, s, action, reward, s_, done)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmem_counter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmem_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Debug store_transition: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'self.state_memory[index]: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_memory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_memory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_state_memory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward_memory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (15,) into shape (8,)"
     ]
    }
   ],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "import pandas as pd\n",
    "import math\n",
    "import openpyxl\n",
    "\n",
    "## Adagrad\n",
    "class DeepQNet(nn.Module):\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, n_actions):\n",
    "        \n",
    "        super(DeepQNet, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        # self.device = T.device('cpu')\n",
    "        self.to(self.device)\n",
    "    def forward(self, s):\n",
    "        \n",
    "      \n",
    "        tmp = F.sigmoid(self.fc1(s))    ##################\n",
    "        tmp = F.sigmoid(self.fc2(tmp))\n",
    "        actions = self.fc3(tmp)\n",
    "        \n",
    "        #print(\"Ende Action\")\n",
    "        return actions\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size,\n",
    "                 n_actions, mem_size=1000000, eps_end=0.05, eps_dec=4e-4):\n",
    "        # print('Class Agent, input_dims: ', input_dims)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_end = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.lr = lr\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.mem_size = mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_counter = 0\n",
    "        #self.anzahl = i\n",
    "        self.Q_eval = DeepQNet(self.lr, input_dims=input_dims, n_actions=n_actions,\n",
    "                               fc1_dims=256, fc2_dims=128)\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
    "    def store_transition(self, s, action, reward, s_, done):\n",
    "        \n",
    "        index = self.mem_counter % self.mem_size\n",
    "        print(\"Debug store_transition: \", s, \"\\n\", 'self.state_memory[index]: ', self.state_memory[index])\n",
    "        self.state_memory[index] = s\n",
    "        self.new_state_memory[index] = s_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_counter += 1\n",
    "        \n",
    "    def choose_action(self, observation, verbleibende_Kapa):\n",
    "        \n",
    "        if verbleibende_Kapa < observation[0]:\n",
    "            \n",
    "            action = 0\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            if np.random.random() > self.epsilon:        # Hier nach das < in  > umtauschen\n",
    "                # In der neuen Variante: nächstes unbearbeitete/unerledigte Tuple + Aktuelle Kapazität an neuronales Netz geben\n",
    "\n",
    "                # Übergeben des Spielfeldes aus der Beobachtung \n",
    "                s = T.tensor([observation.astype(np.single)]).to(self.Q_eval.device)\n",
    "                #print('State s _choose_action_ :')\n",
    "                #print(s)\n",
    "\n",
    "                # Neue Variante: Neuronales Netz gibt gewichtetes Ja und Nein Zurück (nächsten Auftrag annehmen oder ablehnen)\n",
    "                # Übergebe das Spielfeld dem Neuronalen Netz -> Bekomme Ja und Nein Zurück mit jeweiligen wahrscheinlichkeiten / Reward / Kosten\n",
    "                actions = self.Q_eval.forward(s)        #   [[-1.0435,  0.6671]]\n",
    "\n",
    "\n",
    "                action = T.argmax(actions).item()        # Entscheide zwischen Ja und nein auf basis wahrscheinlichkeiten / Reward / Kosten\n",
    "\n",
    "            else:\n",
    "                action = np.random.choice(self.action_space)\n",
    "\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def choose_action_Abwechlung(self, observation):                   \n",
    "\n",
    "        action = np.random.choice(self.action_space)\n",
    "\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def learn(self):\n",
    "        \n",
    "        if self.mem_counter < self.batch_size:\n",
    "            #print(\"XCbyc\")\n",
    "            return\n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "        #self.anzahl = i\n",
    "        \n",
    "        max_mem = min(self.mem_counter, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "        action_batch = self.action_memory[batch]\n",
    "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
    "        q_next = self.Q_eval.forward(new_state_batch)\n",
    "        q_next[terminal_batch] = 0.0\n",
    "        q_target = reward_batch + self.gamma * T.max(q_next, dim=1)[0]\n",
    "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "        #l1.append(loss.item())\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "        \n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_end \\\n",
    "            else self.eps_end\n",
    "    def save_model(self, path):\n",
    "        T.save({\n",
    "            'model_state_dict': self.Q_eval.state_dict(),\n",
    "            'optimizer_state_dict': self.Q_eval.optimizer.state_dict()\n",
    "        }, path)\n",
    "    def load_model(self, path):\n",
    "        checkpoint = T.load(path)\n",
    "        self.Q_eval.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.Q_eval.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.Q_eval.to(self.Q_eval.device)\n",
    " \n",
    "class Wendtris(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    # def __init__(self, *args, **kwargs):\n",
    "    # self.reset( *args, **kwargs)\n",
    "    def __init__(self, episode_length=10, \\\n",
    "                 ressource_capacity=30, min_rev=1, max_rev=9, legacy_model=False):\n",
    "        super(Wendtris, self)\n",
    "        self.episode_length = episode_length\n",
    "        #self.ressource_types = ressource_types\n",
    "        self.ressource_capacity = ressource_capacity\n",
    "        self.min_rev = min_rev\n",
    "        self.max_rev = max_rev\n",
    "        self.legacy_model = legacy_model\n",
    "        self.Auftraege_pro_Epoche \n",
    "        \n",
    "        self.action_space = [0, 1]  ## 0-decline 1-accept\n",
    "        \n",
    "        # Dateipfad zur Excel-Datei angeben\n",
    "        excel_file = 'Testinstanz.xlsx'\n",
    "\n",
    "        # Daten aus der Excel-Datei in ein pandas DataFrame laden\n",
    "        data = pd.read_excel(excel_file)\n",
    "\n",
    "        # Liste initialisieren, um die Daten zu speichern\n",
    "        self.Alle_Auftraege_insgesammt= []\n",
    "\n",
    "        # Schleife, um die Zeilen der Excel-Datei zu durchlaufen\n",
    "        for i, row in data.iterrows():\n",
    "            # Wenn die Anzahl der Zeilen in der Liste episode_length erreicht hat, eine neue Liste hinzufügen\n",
    "            if i % self.episode_length == 0:\n",
    "                self.Alle_Auftraege_insgesammt.append([])\n",
    "            # Daten aus der aktuellen Zeile hinzufügen\n",
    "            self.Alle_Auftraege_insgesammt[-1].append(row.tolist())\n",
    "       \n",
    "        self.Anzahl_der_Epochen = len(self.Alle_Auftraege_insgesammt)\n",
    "        #print(self.Alle_Auftraege_insgesammt)\n",
    "        \n",
    "    def reset_all(self):\n",
    "        \n",
    "        \n",
    "        self.current_step = 0\n",
    "        self.recent_reward = 0\n",
    "        self.total_reward = 0\n",
    "        self.total_lost_reward = 0\n",
    "        self.requests_accepted = 0\n",
    "        self.requests_declined = 0\n",
    "        self.state_capacity = 30\n",
    "        self.vorheriges_X = 700\n",
    "        self.vorheriges_Y = 99\n",
    "        self.done = True \n",
    "        print(\"reset_all\")\n",
    "        self.Belohnung = []\n",
    "        self.Kapazitaet = []\n",
    "        self.Koordinate = []\n",
    "        \n",
    "        return(self.Alle_Auftraege_insgesammt)\n",
    " \n",
    "    def Auftraege_pro_Epoche(self, i):\n",
    "        \n",
    "        Auftraege_pro_Epoche = self.Alle_Auftraege_insgesammt[i]\n",
    "        #print(\"Epoche\")\n",
    "        #print(i)\n",
    "        #print(Auftraege_pro_Epoche)\n",
    "        return(self.Auftraege_pro_Epoche)    \n",
    "            \n",
    "    def reset(self):\n",
    "        # ERzeuge neues Spielfeld, evtl. initiale Kapazität und initialen Standpunkt\n",
    "        # [(R, x,y, K), (R,x,y,K),..]\n",
    "        #print(\"Reset:\")\n",
    "        \n",
    "        \n",
    "       \n",
    "       # self.current_step = 0\n",
    "        #self.recent_reward = 0\n",
    "      #  self.total_reward = 0\n",
    "       # self.requests_accepted = 0\n",
    "       # self.requests_declined = 0\n",
    "       # self.state_capacity = np.ones(self.ressource_types, int) * self.ressource_capacity\n",
    "       # self.done = False\n",
    "        #self.request = np.zeros((self.episode_length, self.ressource_types), int)\n",
    "        #self.request_type = np.random.randint(0, \\\n",
    "       #                                       len(self.base_request_set), self.episode_length, int)\n",
    "       # self.request_reward_per_unit = np.random.randint(self.min_rev, \\\n",
    "#                                                         self.max_rev + 1, self.episode_length, int)\n",
    "       # self.request_reward = np.zeros(self.episode_length, int)\n",
    "       # self.request_position = np.zeros(self.episode_length, int)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        self.current_step = 0\n",
    "        self.recent_reward = 0\n",
    "        self.total_reward = 0\n",
    "        self.total_lost_reward = 0\n",
    "        self.requests_accepted = 0\n",
    "        self.requests_declined = 0\n",
    "        self.state_capacity = 30\n",
    "        self.vorheriges_X = 700\n",
    "        self.vorheriges_Y = 99\n",
    "        self.done = True \n",
    "        #Liste_Belohnungen = \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        self.Auftrag = self.Alle_Auftraege_insgesammt[0][Epoche]\n",
    "        \n",
    "        self.Belohnung = []\n",
    "        self.Kapazitaet = []\n",
    "        self.Koordinate = []\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        for Auftrag in self.Alle_Auftraege_insgesammt:\n",
    "                c_Belohnung = np.array([sub_arr[1] for sub_arr in Auftrag])\n",
    "                self.Belohnung.append(c_Belohnung)\n",
    "                tensor = np.array([sub_arr[0] for sub_arr in Auftrag])\n",
    "                self.Kapazitaet.append(tensor)\n",
    "                x_koordinate = np.array([sub_arr[2] for sub_arr in Auftrag])\n",
    "                y_koordinate = np.array([sub_arr[3] for sub_arr in Auftrag])\n",
    "\n",
    "                kord = [x_koordinate,y_koordinate]\n",
    "                #print(kord)\n",
    "                self.Koordinate.append(kord)\n",
    "      \n",
    "            \n",
    "        #print(self.Alle_Auftraege_insgesammt)\n",
    "    #    print(self.Belohnung)\n",
    "      #  print(self.Koordinate)\n",
    "        #print(\"Kapa: \")\n",
    "        #print(self.Kapazitaet)\n",
    "        \n",
    "        \n",
    "        return (self.observe())\n",
    "    \n",
    "    \n",
    "    def observe(self, Auftrag, temp_Auftragsnummer, verbleibende_Kapa, aktuelles_X, aktuelles_Y ):\n",
    "        \n",
    "        #self.observation = np.append(self.Alle_Auftraege_insgesammt[self.current_step], self.state_capacity)\n",
    "        \n",
    "        \n",
    "        # einfach das erzeuge Spielfeld + aktuelle Position + aktuell verbrauchte Kapazität(schrumpft mit jedem angennommenen Auftrag) zurückgeben\n",
    "        self.observation = np.append(Auftrag, verbleibende_Kapa)\n",
    "\n",
    "        requests_remaining = self.episode_length - temp_Auftragsnummer\n",
    "        \n",
    "        #speichert nach observation noch die verbleibende Anzahl von Aufträgen, die vorherige X und Y koordinate\n",
    "        self.observation = np.append(self.observation, [requests_remaining, \\\n",
    "                                                        aktuelles_X, aktuelles_Y])\n",
    "\n",
    "        return self.observation\n",
    "    \n",
    "     \n",
    "    def step(self,action,observation, Auftrag,verbleibende_Kapa,Auftrags_nummer,aktuelles_X, aktuelles_Y):\n",
    "        \n",
    "\n",
    "        # Hier wird die Kapazität abgezogen\n",
    "\n",
    "        \n",
    "        #Hier kommt die Abfrage nach der Action hin If action = 1 reward += ....\n",
    "       \n",
    "        temp_Auftragsnummer =  Auftrags_nummer + 1\n",
    "\n",
    "        \n",
    "        \n",
    "        if action == 1:\n",
    "            \n",
    "            verbleibende_Kapa -=  observation[0]\n",
    "            \n",
    "\n",
    "        observation = env.observe(Auftrag, temp_Auftragsnummer, verbleibende_Kapa, aktuelles_X, aktuelles_Y )\n",
    "        \n",
    "\n",
    "        return observation, self.done, '', verbleibende_Kapa\n",
    "    \n",
    "\n",
    "env = Wendtris(legacy_model=False)\n",
    "## Hier die InputDimensionanpassen\n",
    "agent = Agent(gamma=0.99, epsilon= 0.0, lr=0.004, input_dims=[8], batch_size=64, n_actions=2, eps_dec=2e-6)\n",
    "print(\"Anfan1g\")\n",
    "\n",
    "Alle_daten = env.reset_all()  \n",
    "Anz_Auftraege_pro_Epoche = len(Alle_daten[1])\n",
    "\n",
    "Vefuegbare_Kapa = 100\n",
    "max_score = 0\n",
    "\n",
    "\n",
    "score_Liste = []\n",
    "#print(\"Alle DAten\")\n",
    "#print(Alle_daten)\n",
    "\n",
    "scores, avg_scores_50, eps_history = [], [], []\n",
    "bisheriger_Best_score = 0\n",
    "\n",
    "agent.load_model(\"Neu_Trainingsdaten_A_Kapa100_tanh_Standart_10_Auftraege.zip\")\n",
    "\n",
    "\n",
    "Liste_score = []\n",
    "Liste_action = []\n",
    "Liste_best_action = []\n",
    "\n",
    "#print(Alle_daten[15])\n",
    "for i in range(100-1):\n",
    "    \n",
    "    print(i)\n",
    "    reward=0\n",
    "    verbleibende_Kapa = Vefuegbare_Kapa\n",
    "    vorheriges_X = 0\n",
    "    vorheriges_Y = 0\n",
    "    aktuelles_X= 0\n",
    "    aktuelles_Y = 0\n",
    "\n",
    "    \n",
    "    Auftraege_pro_Epoche = Alle_daten[i]\n",
    "    \n",
    "    \n",
    "    Ursprung = (0,0)\n",
    "    \n",
    "    score = 0\n",
    "    \n",
    "    max_score = 0\n",
    "  \n",
    "    Auftrags_nummer = 0  # Nummer des jeweiligen auftrags\n",
    "    \n",
    "  \n",
    "    observation = env.observe(Auftraege_pro_Epoche[0], Auftrags_nummer, verbleibende_Kapa, aktuelles_X, aktuelles_Y )\n",
    "    #print(\"observation zu beginn\")\n",
    "    print(observation)\n",
    "    zähler = 0\n",
    "    Liste_action = []\n",
    "    \n",
    "    while Auftrags_nummer < Anz_Auftraege_pro_Epoche:\n",
    "        \n",
    "        zähler+=1 \n",
    "        Auftrag = Auftraege_pro_Epoche[Auftrags_nummer]\n",
    "        #Auftrag_next = Auftraege_pro_Epoche[Auftrags_nummer+1]\n",
    "        #print(Auftrag)\n",
    "        \n",
    "        \n",
    "        action = agent.choose_action(observation, verbleibende_Kapa )\n",
    "       # print(\"Action\")\n",
    "        #print(action)\n",
    "        Liste_action.append(action)\n",
    "        \n",
    "        if action == 1:\n",
    "            \n",
    "            aktuelles_X = observation[2]\n",
    "            aktuelles_Y = observation[3]\n",
    "            Aktueller_Punkt_ = (observation[6],observation[7])\n",
    "        \n",
    "            belohnung_new = observation[1]\n",
    "\n",
    "            Kosten = math.dist(Aktueller_Punkt_,(observation[2],observation[3]))\n",
    "\n",
    "            reward_new = belohnung_new - Kosten \n",
    "\n",
    "            score += reward_new\n",
    "            \n",
    "            if Auftrags_nummer == Anz_Auftraege_pro_Epoche-1:\n",
    "                Auftrag = [0,0,0,0]\n",
    "            else:\n",
    "                Auftrag = Auftraege_pro_Epoche[Auftrags_nummer+1]\n",
    "\n",
    "         # Es muss noch eine Anpassung der verbleibenden Aufträge stattfinden\n",
    "            # temp_Aufträge = Auftragsnummer -1\n",
    "            # Diese sollen dann in die Step Funktion weiter gegeben werden\n",
    "        observation_, done, info ,verbleibende_Kapa = env.step(action,observation,Auftrag,verbleibende_Kapa,\\\n",
    "                                                               Auftrags_nummer,aktuelles_X, aktuelles_Y)\n",
    "        \n",
    "\n",
    "        \n",
    "        print(observation)\n",
    "\n",
    "        \n",
    "        if Auftrags_nummer == Anz_Auftraege_pro_Epoche-1 :\n",
    "            \n",
    "            \n",
    "            #print(\"Score\")\n",
    "            #print(score)\n",
    "            endpunkt = (aktuelles_X,aktuelles_Y)\n",
    "            #print(endpunkt)\n",
    "            score -= math.dist(Ursprung, endpunkt)\n",
    "            #print(score)\n",
    "\n",
    "\n",
    "            \n",
    "        agent.store_transition(observation, action, \\\n",
    "                               score, observation_, done)        \n",
    "        \n",
    "        \n",
    "        agent.learn()\n",
    "            \n",
    "        observation = observation_\n",
    "\n",
    "        \n",
    "        \n",
    "        Auftrags_nummer +=1 \n",
    "        \n",
    "        \n",
    "    if score > max_score:\n",
    "            max_score = score\n",
    "            \n",
    "            \n",
    "    if max_score > bisheriger_Best_score:\n",
    "            bisheriger_Best_score = max_score   \n",
    "        \n",
    "        \n",
    "    Liste_score.append(score)\n",
    "        \n",
    "    Liste_best_action.append(Liste_action)\n",
    "    \n",
    "    \n",
    "print(Alle_daten[3])\n",
    "print(Liste_score[3])\n",
    "    \n",
    "#print(Liste_action)\n",
    "#print(Liste_best_action)\n",
    "\n",
    "    #np.savetxt('scores_and_eps_Trainingsdaten_A_Kapa100_sigmoid_Standart_10_Auftraege.csv', history, delimiter=',', \\\n",
    "               #header=\"scores,avg_scores_50,epsilon\", fmt='%f')\n",
    "\n",
    "        \n",
    "# Neue Excel-Datei erstellen\n",
    "wb = openpyxl.Workbook()\n",
    "\n",
    "# Arbeitsblatt auswählen\n",
    "ws = wb.active\n",
    "\n",
    "# Überschriften für die Spalten setzen\n",
    "ws['A1'] = 'Score'\n",
    "for col in range(2, 12):\n",
    "    ws.cell(row=1, column=col).value = f'Action {col-1}'\n",
    "\n",
    "# Daten in die Excel-Tabelle schreiben\n",
    "for row in range(2, 32):\n",
    "    # Score-Wert in die erste Spalte schreiben\n",
    "    ws.cell(row=row, column=1).value = Liste_score[row-2]\n",
    "    \n",
    "    # Action-Werte in die restlichen Spalten schreiben\n",
    "    for col in range(2, 12):\n",
    "        ws.cell(row=row, column=col).value = Liste_best_action[row-2][col-2]\n",
    "\n",
    "# Excel-Datei speichern\n",
    "wb.save('Neu_Trainingsdaten_A_Kapa100_tanh_Standart_10_Auftraege.xlsx')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f9f0c82",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4012691780.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\DanielDHerzog\\AppData\\Local\\Temp\\ipykernel_20840\\4012691780.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    echo \"# neuronales_Netz\" >> README.md\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa133ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
